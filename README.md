1.我们要掌握Linux的安装及基本操作、Python安装及编程基础、java基础。需要学习Linux的常用命令、基本网络配置、进程管理、shell语法;Python的常用语法，能够基于Python搭建一个常用的Server服务器和java的基础知识。这时候只需要掌握基础即可，后边遇到问题再学习，这样才不会混乱，学的才扎实。2. 搭建Hadoop分布式环境我们要做的是在自己的电脑上安装Linux，然后准备环境nat配置，搭建Hadoop集群先让Hadoop在自己的电脑上跑起来。使用VMware来搭建。这时候我们会Host配置、IP配置、SSH免密登录等。3.学习HDFS分布式文件系统这一步要学习架构分析、容灾容错策略、local数据策略、数据块概念、机架感应，功能逻辑实现等。要真正的去敲敲，掌握Linux下HDFS Shell常用命令的使用。4.学习MapReduce计算框架MapReduce是Hadoop核心编程模型。在Hadoop中，数据处理核心就是MapReduce程序设计模型。这一步需要学的东西很多，大家一定要有耐心，把MR的知识学牢固。首先我们需要学习MR的基本原理、任务执行流程、Shuffle策略。自己动手写一个MR任务，来实现wordcount。然后要学习表单join、表单查询、数据清洗、全局排序、多目录输入输出、自定义partition分区，掌握二分法算法。接下来学习自然语言处理方法(NLP)，掌握如何提取关键词，TF-IDF算法。这里我们可以实践一下，统计文本中的词频。学习中文分词，分词的质量直接影响数据挖掘的质量。5.学习Strom流式计算Storm是一个开源分布式实时计算系统，它可以实时可靠地处理流数据。这一步我们要知道Hadoop和Storm的区别，知道他们如何进行互补。了解Storm的体系架构、Zookeeper在架构中的作用和数据流处理的过程。弄懂Storm的工作原理和核心组件(Spout、Bolt)6.学习Zookeeper分布式协作服务这一步我们学会数据管理的树形结构，学会根据应用场景选择不同类型的节点、节点权限管理ACL和监控机制。学会Zookeeper开源自带Client工具的Shell使用，开发java代码实现不同类型的节点进行新建、修改、删除和节点的监控。7.学习数据仓库工具Hive这一步要了解Hive的体系架构和其与mysql的对比。要掌握Mysql的基本知识、系统搭建标准SQL语(增删查改)。8.学习分布式存储系统Hbase这一步要掌握Hbase的体系架构(HMaster、HRegionServer、HStore、HFile、HLog)，物理存储、数据逻辑存储、核心功能模块。细化一点要掌握Hbase表结构设计、Shell操作(增删查改)、javaAPI操作、数据迁移、备份与恢复。与MR结合实现批量导入与导出，与Hive结合使用，集群管理和性能调优。9.学习Spark这一步要掌握SPark的编程模型、运行框架、作业提交、缓存策略、RDD、MLLib。10.学习Scala语言这一步要掌握Scala的常用语法、函数、元组等操作，不熟Spark。11.学习Spark开发技术这一步要能够熟练使用MLLib，能够自己开发Scala的Spark任务，完成表格join、连接和文本串过滤等。12.学习推荐系统前面我们学了那么多，最终所学的技术要能落地，我学的是现在主流的推荐系统，现在各大公司都需要这方面的人才。这一步我们可以找一些案例在学习，要掌握主流的推荐算法，Content Base、Collab Filter。a.学习基于MR的协同过滤算法b.学习Mahout，掌握Mahout的适用场景、环境搭建与部署，学习基于Mahout的协同过滤算法，与MR进行效果对比。C.学习基于Spark的协同过滤算法到这里，按照上边的路线认真学习，肯定能学好hadoop开发，在学习的时候一定要亲自动手去敲，要去不断的尝试，把看到的知识尽快转化为自己的技能，这样才能高效率的学会hadoop，学任何一门技术都是一样，需要实际动手。

作者：嘿你好夏天
链接：https://juejin.im/post/5ac19986f265da238f12b36e
来源：掘金
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
